{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw_walking normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal', \n",
    "    '2) walking fast', \n",
    "    '3) weight normal', \n",
    "    '4) weight fast',\n",
    "    '5) brace normal', \n",
    "    '6) brace fast', \n",
    "    '7) brace_weight normal', \n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'RAW_walking normal.csv', \n",
    "    'RAW_walking fast.csv', \n",
    "    'RAW_weight normal.csv', \n",
    "    'RAW_weight fast.csv',\n",
    "    'RAW_brace normal.csv', \n",
    "    'RAW_brace fast.csv', \n",
    "    'RAW_brace weight normal.csv', \n",
    "    'RAW_brace weight fast.csv'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "# Define the desired column order\n",
    "desired_column_order = [\n",
    "    'Time_LeftFoot', 'AccX_LeftFoot', 'AccY_LeftFoot', 'AccZ_LeftFoot', 'GyroX_LeftFoot', 'GyroY_LeftFoot', 'GyroZ_LeftFoot', 'MagX_LeftFoot', 'MagY_LeftFoot', 'MagZ_LeftFoot',\n",
    "    'Time_RightFoot', 'AccX_RightFoot', 'AccY_RightFoot', 'AccZ_RightFoot', 'GyroX_RightFoot', 'GyroY_RightFoot', 'GyroZ_RightFoot', 'MagX_RightFoot', 'MagY_RightFoot', 'MagZ_RightFoot',\n",
    "    'Time_LeftShank', 'AccX_LeftShank', 'AccY_LeftShank', 'AccZ_LeftShank', 'GyroX_LeftShank', 'GyroY_LeftShank', 'GyroZ_LeftShank', 'MagX_LeftShank', 'MagY_LeftShank', 'MagZ_LeftShank',\n",
    "    'Time_RightShank', 'AccX_RightShank', 'AccY_RightShank', 'AccZ_RightShank', 'GyroX_RightShank', 'GyroY_RightShank', 'GyroZ_RightShank', 'MagX_RightShank', 'MagY_RightShank', 'MagZ_RightShank',\n",
    "    'Time_LeftThigh', 'AccX_LeftThigh', 'AccY_LeftThigh', 'AccZ_LeftThigh', 'GyroX_LeftThigh', 'GyroY_LeftThigh', 'GyroZ_LeftThigh', 'MagX_LeftThigh', 'MagY_LeftThigh', 'MagZ_LeftThigh',\n",
    "    'Time_RightThigh', 'AccX_RightThigh', 'AccY_RightThigh', 'AccZ_RightThigh', 'GyroX_RightThigh', 'GyroY_RightThigh', 'GyroZ_RightThigh', 'MagX_RightThigh', 'MagY_RightThigh', 'MagZ_RightThigh',\n",
    "    'Time_LeftHumerus', 'AccX_RightHumerus', 'AccY_RightHumerus', 'AccZ_RightHumerus', 'GyroX_LeftHumerus', 'GyroY_LeftHumerus', 'GyroZ_LeftHumerus', 'MagX_LeftHumerus', 'MagY_LeftHumerus', 'MagZ_LeftHumerus',\n",
    "    'Time_RightHumerus', 'AccX_RightHumerus', 'AccY_RightHumerus', 'AccZ_RightHumerus', 'GyroX_RightHumerus', 'GyroY_RightHumerus', 'GyroZ_RightHumerus', 'MagX_RightHumerus', 'MagY_RightHumerus', 'MagZ_RightHumerus',\n",
    "    'Time_Pelvic', 'AccX_Pelvic', 'AccY_Pelvic', 'AccZ_Pelvic', 'GyroX_Pelvic', 'GyroY_Pelvic', 'GyroZ_Pelvic', 'MagX_Pelvic', 'MagY_Pelvic', 'MagZ_Pelvic',\n",
    "    'Time_Trunk', 'AccX_Trunk', 'AccY_Trunk', 'AccZ_Trunk', 'GyroX_Trunk', 'GyroY_Trunk', 'GyroZ_Trunk', 'MagX_Trunk', 'MagY_Trunk', 'MagZ_Trunk'\n",
    "]\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(base_dir, 'Data', sub_name, condition).replace('\\\\', '/')\n",
    "        output_dir = os.path.join(base_dir, 'modified data', subject, condition.replace(') ', '_'))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        input_file_path = os.path.join(input_dir, input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            first_row = pd.DataFrame(data.iloc[0]).index\n",
    "            words = list(str(word).strip() for word in first_row)\n",
    "            names = [\n",
    "                word for word in words if word and not re.search(r'\\d', word)]\n",
    "            new_col_names = []\n",
    "\n",
    "            for i in range(len(first_row)):\n",
    "                name = names[i // 10]\n",
    "                sensor = ['Time', 'AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ', 'MagX', 'MagY', 'MagZ'][i % 10]\n",
    "                new_col_names.append(f'{sensor}_{name}')\n",
    "\n",
    "            data.columns = new_col_names\n",
    "            data = data.iloc[1:]\n",
    "\n",
    "            # Sort the columns using the desired column order\n",
    "            data = data.drop_duplicates(subset=['Time_LeftFoot'], keep='first')\n",
    "            data = data.reindex(columns=desired_column_order)\n",
    "\n",
    "            output_file_name = input_file_name\n",
    "            output_file = os.path.join(output_dir, output_file_name)\n",
    "            data.to_csv(output_file, index=False)\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walking normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal.csv',\n",
    "    'walking fast.csv',\n",
    "    'weight normal.csv',\n",
    "    'weight fast.csv',\n",
    "    'brace normal.csv',\n",
    "    'brace fast.csv',\n",
    "    'brace weight normal.csv',\n",
    "    'brace weight fast.csv'\n",
    "]\n",
    "\n",
    "desired_column_order = ['Time_LeftFoot', 'Q0_LeftFoot', 'Q1_LeftFoot', 'Q2_LeftFoot', 'Q3_LeftFoot', 'Acc_X_LeftFoot', 'Acc_Y_LeftFoot', 'Acc_Z_LeftFoot', 'Acc_linX_LeftFoot', 'Acc_linY_LeftFoot', 'Acc_linZ_LeftFoot', 'Acc_GlinX_LeftFoot', 'Acc_GlinY_LeftFoot', 'Acc_GlinZ_LeftFoot',\n",
    "                        'Time_RightFoot', 'Q0_RightFoot', 'Q1_RightFoot', 'Q2_RightFoot', 'Q3_RightFoot', 'Acc_X_RightFoot', 'Acc_Y_RightFoot', 'Acc_Z_RightFoot', 'Acc_linX_RightFoot', 'Acc_linY_RightFoot', 'Acc_linZ_RightFoot', 'Acc_GlinX_RightFoot', 'Acc_GlinY_RightFoot', 'Acc_GlinZ_RightFoot',\n",
    "                        'Time_LeftShank', 'Q0_LeftShank', 'Q1_LeftShank', 'Q2_LeftShank', 'Q3_LeftShank', 'Acc_X_LeftShank', 'Acc_Y_LeftShank', 'Acc_Z_LeftShank', 'Acc_linX_LeftShank', 'Acc_linY_LeftShank', 'Acc_linZ_LeftShank', 'Acc_GlinX_LeftShank', 'Acc_GlinY_LeftShank', 'Acc_GlinZ_LeftShank',\n",
    "                        'Time_RightShank', 'Q0_RightShank', 'Q1_RightShank', 'Q2_RightShank', 'Q3_RightShank', 'Acc_X_RightShank', 'Acc_Y_RightShank', 'Acc_Z_RightShank', 'Acc_linX_RightShank', 'Acc_linY_RightShank', 'Acc_linZ_RightShank', 'Acc_GlinX_RightShank', 'Acc_GlinY_RightShank', 'Acc_GlinZ_RightShank',\n",
    "                        'Time_LeftThigh', 'Q0_LeftThigh', 'Q1_LeftThigh', 'Q2_LeftThigh', 'Q3_LeftThigh', 'Acc_X_LeftThigh', 'Acc_Y_LeftThigh', 'Acc_Z_LeftThigh', 'Acc_linX_LeftThigh', 'Acc_linY_LeftThigh', 'Acc_linZ_LeftThigh', 'Acc_GlinX_LeftThigh', 'Acc_GlinY_LeftThigh', 'Acc_GlinZ_LeftThigh',\n",
    "                        'Time_RightThigh', 'Q0_RightThigh', 'Q1_RightThigh', 'Q2_RightThigh', 'Q3_RightThigh', 'Acc_X_RightThigh', 'Acc_Y_RightThigh', 'Acc_Z_RightThigh', 'Acc_linX_RightThigh', 'Acc_linY_RightThigh', 'Acc_linZ_RightThigh', 'Acc_GlinX_RightThigh', 'Acc_GlinY_RightThigh', 'Acc_GlinZ_RightThigh',\n",
    "                        'Time_LeftHumerus', 'Q0_LeftHumerus', 'Q1_LeftHumerus', 'Q2_LeftHumerus', 'Q3_LeftHumerus', 'Acc_X_LeftHumerus', 'Acc_Y_LeftHumerus', 'Acc_Z_LeftHumerus', 'Acc_linX_LeftHumerus', 'Acc_linY_LeftHumerus', 'Acc_linZ_LeftHumerus', 'Acc_GlinX_LeftHumerus', 'Acc_GlinY_LeftHumerus', 'Acc_GlinZ_LeftHumerus',\n",
    "                        'Time_RightHumerus', 'Q0_RightHumerus', 'Q1_RightHumerus', 'Q2_RightHumerus', 'Q3_RightHumerus', 'Acc_X_RightHumerus', 'Acc_Y_RightHumerus', 'Acc_Z_RightHumerus', 'Acc_linX_RightHumerus', 'Acc_linY_RightHumerus', 'Acc_linZ_RightHumerus', 'Acc_GlinX_RightHumerus', 'Acc_GlinY_RightHumerus', 'Acc_GlinZ_RightHumerus',\n",
    "                        'Time_Pelvic', 'Q0_Pelvic', 'Q1_Pelvic', 'Q2_Pelvic', 'Q3_Pelvic', 'Acc_X_Pelvic', 'Acc_Y_Pelvic', 'Acc_Z_Pelvic', 'Acc_linX_Pelvic', 'Acc_linY_Pelvic', 'Acc_linZ_Pelvic', 'Acc_GlinX_Pelvic', 'Acc_GlinY_Pelvic', 'Acc_GlinZ_Pelvic',\n",
    "                        'Time_Trunk', 'Q0_Trunk', 'Q1_Trunk', 'Q2_Trunk', 'Q3_Trunk', 'Acc_X_Trunk', 'Acc_Y_Trunk', 'Acc_Z_Trunk', 'Acc_linX_Trunk', 'Acc_linY_Trunk', 'Acc_linZ_Trunk', 'Acc_GlinX_Trunk', 'Acc_GlinY_Trunk', 'Acc_GlinZ_Trunk'\n",
    "                        ]\n",
    "\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(\n",
    "            base_dir, 'Data', sub_name, condition).replace('\\\\', '/')\n",
    "        output_dir = os.path.join(\n",
    "            base_dir, 'modified data', subject, condition.replace(') ', '_'))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        input_file_path = os.path.join(input_dir, input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            first_row = pd.DataFrame(data.iloc[0]).index\n",
    "            words = list(str(word).strip() for word in first_row)\n",
    "            names = [\n",
    "                word for word in words if word and not re.search(r'\\d', word)]\n",
    "            new_col_names = []\n",
    "\n",
    "            for i in range(len(first_row)):\n",
    "                name = names[i // 14]\n",
    "                sensor = ['Time', 'Q0', 'Q1', 'Q2', 'Q3', 'Acc_X', 'Acc_Y', 'Acc_Z',\n",
    "                          'Acc_linX', 'Acc_linY', 'Acc_linZ', 'Acc_GlinX', 'Acc_GlinY', 'Acc_GlinZ'][i % 14]  # Get the sensor name\n",
    "                new_col_names.append(f'{sensor}_{name}')\n",
    "\n",
    "            data.columns = new_col_names\n",
    "            data = data.iloc[1:]\n",
    "\n",
    "            # Sort the columns using the desired column order\n",
    "            data = data.drop_duplicates(subset=['Time_LeftFoot'], keep='first')\n",
    "            data = data.reindex(columns=desired_column_order)\n",
    "\n",
    "            output_file_name = input_file_name\n",
    "            output_file = os.path.join(output_dir, output_file_name)\n",
    "            data.to_csv(output_file, index=False)\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walking normal Euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m input_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, input_file_name)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(input_file_path):\n\u001b[1;32m---> 66\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     first_row \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Convert the index to a list of strings\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arash\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arash\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Arash\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arash\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Arash\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "File \u001b[1;32mc:\\Users\\Arash\\AppData\\Local\\Programs\\Python\\Python39\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;66;03m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal_Euler.csv',\n",
    "    'walking fast_Euler.csv',\n",
    "    'weight normal_Euler.csv',\n",
    "    'weight fast_Euler.csv',\n",
    "    'brace normal_Euler.csv',\n",
    "    'brace fast_Euler.csv',\n",
    "    'brace weight normal_Euler.csv',\n",
    "    'brace weight fast_Euler.csv'\n",
    "]\n",
    "\n",
    "desired_column_order = ['Time_LeftFoot', 'Roll_LeftFoot', 'Pitch_LeftFoot', 'Yaw_LeftFoot',\n",
    "                        'Time_RightFoot', 'Roll_RightFoot', 'Pitch_RightFoot', 'Yaw_RightFoot',\n",
    "                        'Time_LeftShank', 'Roll_LeftShank', 'Pitch_LeftShank', 'Yaw_LeftShank',\n",
    "                        'Time_RightShank', 'Roll_RightShank', 'Pitch_RightShank', 'Yaw_RightShank',\n",
    "                        'Time_LeftThigh', 'Roll_LeftThigh', 'Pitch_LeftThigh', 'Yaw_LeftThigh',\n",
    "                        'Time_RightThigh', 'Roll_RightThigh', 'Pitch_RightThigh', 'Yaw_RightThigh',\n",
    "                        'Time_LeftHumerus', 'Roll_LeftHumerus', 'Pitch_LeftHumerus', 'Yaw_LeftHumerus',\n",
    "                        'Time_RightHumerus', 'Roll_RightHumerus', 'Pitch_RightHumerus', 'Yaw_RightHumerus',\n",
    "                        'Time_Pelvic', 'Roll_Pelvic', 'Pitch_Pelvic', 'Yaw_Pelvic',\n",
    "                        'Time_Trunk', 'Roll_Trunk', 'Pitch_Trunk', 'Yaw_Trunk'\n",
    "                        ]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(base_dir, 'Data', sub_name, condition).replace('\\\\', '/')\n",
    "        output_dir = os.path.join(base_dir, 'modified data', subject, condition.replace(') ', '_'))\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        input_file_path = os.path.join(input_dir, input_file_name).replace('\\\\', '/')\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            first_row = pd.DataFrame(data.iloc[0]).index\n",
    "\n",
    "            # Convert the index to a list of strings\n",
    "            words = list(str(word).strip() for word in first_row)\n",
    "\n",
    "            # Remove empty strings and strings containing numbers\n",
    "            names = [word for word in words if word and not re.search(r'\\d', word)]\n",
    "\n",
    "            # Create a list of column names\n",
    "            new_col_names = []\n",
    "            for i in range(len(first_row)):\n",
    "                name = names[i // 4]  # Get the corresponding name\n",
    "                sensor = ['Time', 'Roll', 'Pitch', 'Yaw'][i % 4]  # Get the sensor name\n",
    "                # Create the new column name\n",
    "                new_col_names.append(f'{sensor}_{name}')\n",
    "\n",
    "            # Rename the columns\n",
    "            data.columns = new_col_names\n",
    "\n",
    "            # Remove the first row\n",
    "            data = data.iloc[1:]\n",
    "            \n",
    "            # Sort the columns using the desired column order\n",
    "            data = data.drop_duplicates(subset=['Time_LeftFoot'], keep='first')\n",
    "            data = data.reindex(columns=desired_column_order)\n",
    "\n",
    "            output_file_name = input_file_name\n",
    "\n",
    "            # Construct the output file path\n",
    "            output_file = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "            # Save the updated DataFrame to the output file\n",
    "            data.to_csv(output_file, index=False)\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walking normal_Joints_Kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Akram Shojaei/1) walking normal/walking normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Akram Shojaei/2) walking fast/walking fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Akram Shojaei/3) weight normal/weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Akram Shojaei/4) weight fast/weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Akram Shojaei/5) brace normal/brace normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Akram Shojaei/6) brace fast/brace fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Akram Shojaei/7) brace_weight normal/brace weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Akram Shojaei/8) brace_weight fast/brace weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Ali Aghapour/1) walking normal/walking normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Ali Aghapour/2) walking fast/walking fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Ali Aghapour/3) weight normal/weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Ali Aghapour/4) weight fast/weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Ali Aghapour/5) brace normal/brace normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Ali Aghapour/6) brace fast/brace fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Ali Aghapour/7) brace_weight normal/brace weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Ali Aghapour/8) brace_weight fast/brace weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Behnaz Behara/1) walking normal/walking normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Behnaz Behara/2) walking fast/walking fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Behnaz Behara/3) weight normal/weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Behnaz Behara/4) weight fast/weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Behnaz Behara/5) brace normal/brace normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Behnaz Behara/6) brace fast/brace fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Behnaz Behara/7) brace_weight normal/brace weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Behnaz Behara/8) brace_weight fast/brace weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Farzad Moghaddam/1) walking normal/walking normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Farzad Moghaddam/2) walking fast/walking fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Farzad Moghaddam/3) weight normal/weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Farzad Moghaddam/4) weight fast/weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Farzad Moghaddam/5) brace normal/brace normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Farzad Moghaddam/6) brace fast/brace fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Farzad Moghaddam/7) brace_weight normal/brace weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Farzad Moghaddam/8) brace_weight fast/brace weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Hasti Keshavarzi/1) walking normal/walking normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Hasti Keshavarzi/2) walking fast/walking fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Hasti Keshavarzi/3) weight normal/weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Hasti Keshavarzi/4) weight fast/weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Hasti Keshavarzi/5) brace normal/brace normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Hasti Keshavarzi/6) brace fast/brace fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Hasti Keshavarzi/7) brace_weight normal/brace weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Hasti Keshavarzi/8) brace_weight fast/brace weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Maliheh Maleki/1) walking normal/walking normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Maliheh Maleki/2) walking fast/walking fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Maliheh Maleki/3) weight normal/weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Maliheh Maleki/4) weight fast/weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Maliheh Maleki/5) brace normal/brace normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Maliheh Maleki/6) brace fast/brace fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Maliheh Maleki/7) brace_weight normal/brace weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Maliheh Maleki/8) brace_weight fast/brace weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Masud Bak Khoshnevis/1) walking normal/walking normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Masud Bak Khoshnevis/2) walking fast/walking fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Masud Bak Khoshnevis/3) weight normal/weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Masud Bak Khoshnevis/4) weight fast/weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Masud Bak Khoshnevis/5) brace normal/brace normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Masud Bak Khoshnevis/6) brace fast/brace fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Masud Bak Khoshnevis/7) brace_weight normal/brace weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Masud Bak Khoshnevis/8) brace_weight fast/brace weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Mobina Jamali/1) walking normal/walking normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Mobina Jamali/2) walking fast/walking fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Mobina Jamali/3) weight normal/weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Mobina Jamali/4) weight fast/weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Mobina Jamali/5) brace normal/brace normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Mobina Jamali/6) brace fast/brace fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Mobina Jamali/7) brace_weight normal/brace weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Mobina Jamali/8) brace_weight fast/brace weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Nadia Khalili/1) walking normal/walking normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Nadia Khalili/2) walking fast/walking fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Nadia Khalili/3) weight normal/weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Nadia Khalili/4) weight fast/weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Nadia Khalili/5) brace normal/brace normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Nadia Khalili/6) brace fast/brace fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Nadia Khalili/7) brace_weight normal/brace weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Nadia Khalili/8) brace_weight fast/brace weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shahab Meghdadi/1) walking normal/walking normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shahab Meghdadi/2) walking fast/walking fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shahab Meghdadi/3) weight normal/weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shahab Meghdadi/4) weight fast/weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shahab Meghdadi/5) brace normal/brace normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shahab Meghdadi/6) brace fast/brace fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shahab Meghdadi/7) brace_weight normal/brace weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shahab Meghdadi/8) brace_weight fast/brace weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shayan Farab/1) walking normal/walking normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shayan Farab/2) walking fast/walking fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shayan Farab/3) weight normal/weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shayan Farab/4) weight fast/weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shayan Farab/5) brace normal/brace normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shayan Farab/6) brace fast/brace fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shayan Farab/7) brace_weight normal/brace weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Shayan Farab/8) brace_weight fast/brace weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Soroush Rezaei/1) walking normal/walking normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Soroush Rezaei/2) walking fast/walking fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Soroush Rezaei/3) weight normal/weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Soroush Rezaei/4) weight fast/weight fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Soroush Rezaei/5) brace normal/brace normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Soroush Rezaei/6) brace fast/brace fast_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Soroush Rezaei/7) brace_weight normal/brace weight normal_Joints_Kinematics.csv\n",
      "Input file not found: D:/Hanieh Moradi - Dataset/Datas/Soroush Rezaei/8) brace_weight fast/brace weight fast_Joints_Kinematics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan Farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal', \n",
    "    '2) walking fast', \n",
    "    '3) weight normal', \n",
    "    '4) weight fast',\n",
    "    '5) brace normal', \n",
    "    '6) brace fast', \n",
    "    '7) brace_weight normal', \n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal_Joints_Kinematics.csv', \n",
    "    'walking fast_Joints_Kinematics.csv',\n",
    "    'weight normal_Joints_Kinematics.csv', \n",
    "    'weight fast_Joints_Kinematics.csv',\n",
    "    'brace normal_Joints_Kinematics.csv', \n",
    "    'brace fast_Joints_Kinematics.csv',\n",
    "    'brace weight normal_Joints_Kinematics.csv', \n",
    "    'brace weight fast_Joints_Kinematics.csv'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "# Define the desired column order\n",
    "desired_column_order = [\n",
    "    'Time_LeftAnkle', 'Abduction-Adduction_LeftAnkle', 'Internal-External Rotat_LeftAnkle', 'Flexion-Extension_LeftAnkle',\n",
    "    'Time_RightAnkle', 'Abduction-Adduction_RightAnkle', 'Internal-External Rotat_RightAnkle', 'Flexion-Extension_RightAnkle',\n",
    "    'Time_LeftKnee', 'Abduction-Adduction_LeftKnee', 'Internal-External Rotat_LeftKnee', 'Flexion-Extension_LeftKnee',\n",
    "    'Time_RightKnee', 'Abduction-Adduction_RightKnee', 'Internal-External Rotat_RightKnee', 'Flexion-Extension_RightKnee',\n",
    "    'Time_LeftHip', 'Abduction-Adduction_LeftHip', 'Internal-External Rotat_LeftHip', 'Flexion-Extension_LeftHip',\n",
    "    'Time_RightHip', 'Abduction-Adduction_RightHip', 'Internal-External Rotat_RightHip', 'Flexion-Extension_RightHip',\n",
    "    'Time_LeftShoulder', 'Abduction-Adduction_LeftShoulder', 'Internal-External Rotat_LeftShoulder', 'Flexion-Extension_LeftShoulder',\n",
    "    'Time_RightShoulder', 'Abduction-Adduction_RightShoulder', 'Internal-External Rotat_RightShoulder', 'Flexion-Extension_RightShoulder',\n",
    "    'Time_Pelvic', 'Abduction-Adduction_Pelvic', 'Internal-External Rotat_Pelvic', 'Flexion-Extension_Pelvic',\n",
    "    'Time_Trunk2Ground', 'Abduction-Adduction_Trunk2Ground', 'Internal-External Rotat_Trunk2Ground', 'Flexion-Extension_Trunk2Ground'\n",
    "]\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(\n",
    "            base_dir, 'Data', sub_name, condition).replace('\\\\', '/')\n",
    "        output_dir = os.path.join(base_dir, 'modified data', subject, condition.replace(') ', '_').replace('\\\\', '/'))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        input_file_path = os.path.join(\n",
    "            input_dir, input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            first_row = pd.DataFrame(data.iloc[0]).index\n",
    "            words = list(str(word).strip() for word in first_row)\n",
    "            names = [word for word in words if (\n",
    "                word == 'Trunk2Ground' or (word and not re.search(r'\\d', word)))]\n",
    "            new_col_names = []\n",
    "\n",
    "            for i in range(len(first_row)):\n",
    "                name = names[i // 4]\n",
    "                sensor = ['Time', 'Abduction-Adduction', 'Internal-External Rotat', 'Flexion-Extension'][i % 4]\n",
    "                new_col_names.append(f'{sensor}_{name}')\n",
    "\n",
    "            data.columns = new_col_names\n",
    "            data = data.iloc[1:]\n",
    "\n",
    "            # Sort the columns based on the desired order\n",
    "            data = data.drop_duplicates(subset=['Time_LeftAnkle'], keep='first')\n",
    "            data = data.reindex(columns=desired_column_order)\n",
    "\n",
    "            output_file_name = input_file_name\n",
    "            output_file = os.path.join(output_dir, output_file_name).replace('\\\\', '/')\n",
    "            data.to_csv(output_file, index=False)\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data - Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan Farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names_1 = [\n",
    "    'RAW_walking normal.csv',\n",
    "    'RAW_walking fast.csv',\n",
    "    'RAW_weight normal.csv',\n",
    "    'RAW_weight fast.csv',\n",
    "    'RAW_brace normal.csv',\n",
    "    'RAW_brace fast.csv',\n",
    "    'RAW_brace weight normal.csv',\n",
    "    'RAW_brace weight fast.csv'\n",
    "]\n",
    "\n",
    "\n",
    "input_file_names_2 = [\n",
    "    'walking normal.csv',\n",
    "    'walking fast.csv',\n",
    "    'weight normal.csv',\n",
    "    'weight fast.csv',\n",
    "    'brace normal.csv',\n",
    "    'brace fast.csv',\n",
    "    'brace weight normal.csv',\n",
    "    'brace weight fast.csv'\n",
    "]\n",
    "\n",
    "input_file_names_3 = [\n",
    "    'walking normal_Euler.csv',\n",
    "    'walking fast_Euler.csv',\n",
    "    'weight normal_Euler.csv',\n",
    "    'weight fast_Euler.csv',\n",
    "    'brace normal_Euler.csv',\n",
    "    'brace fast_Euler.csv',\n",
    "    'brace weight normal_Euler.csv',\n",
    "    'brace weight fast_Euler.csv'\n",
    "]\n",
    "\n",
    "input_file_names_4 = [\n",
    "    'walking normal_Joints_Kinematics.csv',\n",
    "    'walking fast_Joints_Kinematics.csv',\n",
    "    'weight normal_Joints_Kinematics.csv',\n",
    "    'weight fast_Joints_Kinematics.csv',\n",
    "    'brace normal_Joints_Kinematics.csv',\n",
    "    'brace fast_Joints_Kinematics.csv',\n",
    "    'brace weight normal_Joints_Kinematics.csv',\n",
    "    'brace weight fast_Joints_Kinematics.csv'\n",
    "]\n",
    "\n",
    "num = 7\n",
    "selected_condition = conditions[num]\n",
    "selected_file_name_1 = input_file_names_1[num]\n",
    "selected_file_name_2 = input_file_names_2[num]\n",
    "selected_file_name_3 = input_file_names_3[num]\n",
    "selected_file_name_4 = input_file_names_4[num]\n",
    "\n",
    "selected_sub = 'sub_12'\n",
    "\n",
    "base_dir = 'C:/Users/Arash/Desktop/Hanieh Moradi - Dataset/'\n",
    "\n",
    "input_dir = os.path.join(base_dir, 'modified data', selected_sub,\n",
    "                         selected_condition.replace(') ', '_').replace('\\\\', '/'))\n",
    "\n",
    "input_filename_1 = os.path.join(input_dir, selected_file_name_1).replace('\\\\', '/')\n",
    "input_filename_2 = os.path.join(input_dir, selected_file_name_2).replace('\\\\', '/')\n",
    "input_filename_3 = os.path.join(input_dir, selected_file_name_3).replace('\\\\', '/')\n",
    "input_filename_4 = os.path.join(input_dir, selected_file_name_4).replace('\\\\', '/')\n",
    "\n",
    "\n",
    "data_1 = pd.read_csv(input_filename_1)\n",
    "data_2 = pd.read_csv(input_filename_2)\n",
    "data_3 = pd.read_csv(input_filename_3)\n",
    "data_4 = pd.read_csv(input_filename_4)\n",
    "\n",
    "\n",
    "plt.plot(data_1['AccY_LeftFoot'])\n",
    "plt.show()\n",
    "plt.plot(data_1['AccY_LeftFoot'].iloc[:1000])\n",
    "plt.show()\n",
    "plt.plot(data_1['AccY_LeftFoot'].iloc[-1000:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_mod = [200:10300]\n",
    "# finish_mod =\n",
    "\n",
    "data_mod_1 = data_1.iloc[:12600]\n",
    "data_mod_1 = data_mod_1.reset_index(drop=True)\n",
    "data_mod_2 = data_2.iloc[:12600]\n",
    "data_mod_2 = data_mod_2.reset_index(drop=True)\n",
    "data_mod_3 = data_3.iloc[:12600]\n",
    "data_mod_3 = data_mod_3.reset_index(drop=True)\n",
    "data_mod_4 = data_4.iloc[:12600]\n",
    "data_mod_4 = data_mod_4.reset_index(drop=True)\n",
    "\n",
    "base_dir_mod = 'C:/Users/Arash/Desktop/Hanieh Moradi - Dataset/dataset_mod/'\n",
    "\n",
    "# Step 1: Construct the Output Directory Path\n",
    "output_dir = os.path.join(base_dir_mod, 'modified data', selected_sub, selected_condition.replace(') ', '_').replace('\\\\', '/'))\n",
    "\n",
    "# Step 2: Ensure the Directory Exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 3: Construct the Output Filename\n",
    "output_filename_1 = os.path.join(output_dir, selected_file_name_1).replace('\\\\', '/')\n",
    "output_filename_2 = os.path.join(output_dir, selected_file_name_2).replace('\\\\', '/')\n",
    "output_filename_3 = os.path.join(output_dir, selected_file_name_3).replace('\\\\', '/')\n",
    "output_filename_4 = os.path.join(output_dir, selected_file_name_4).replace('\\\\', '/')\n",
    "\n",
    "# Now, you can proceed to save your data to output_filename\n",
    "# For example, if 'data' is your DataFrame:\n",
    "data_mod_1.to_csv(output_filename_1, index=False)\n",
    "data_mod_2.to_csv(output_filename_2, index=False)\n",
    "data_mod_3.to_csv(output_filename_3, index=False)\n",
    "data_mod_4.to_csv(output_filename_4, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the elements of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'RAW_walking normal.csv',\n",
    "    'RAW_walking fast.csv',\n",
    "    'RAW_weight normal.csv',\n",
    "    'RAW_weight fast.csv',\n",
    "    'RAW_brace normal.csv',\n",
    "    'RAW_brace fast.csv',\n",
    "    'RAW_brace weight normal.csv',\n",
    "    'RAW_brace weight fast.csv'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "result_data = []\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(\n",
    "            base_dir, 'Datas', sub_name, condition).replace('\\\\', '/')\n",
    "        input_file_path = os.path.join(input_dir, input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            third_element = int(data.iloc[1, 0])\n",
    "            last_element = int(data.iloc[-1, 0])\n",
    "            length_of_first_column = int(len(data.iloc[:, 0]))\n",
    "            result_data.append(\n",
    "                [subject, condition, third_element, last_element, length_of_first_column])\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=[\n",
    "                         'Subject', 'Condition', 'time_start', 'time_finish', 'length'])\n",
    "result_file = os.path.join(base_dir, 'result_raw.xlsx')\n",
    "result_df.to_excel(result_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal.csv',\n",
    "    'walking fast.csv',\n",
    "    'weight normal.csv',\n",
    "    'weight fast.csv',\n",
    "    'brace normal.csv',\n",
    "    'brace fast.csv',\n",
    "    'brace weight normal.csv',\n",
    "    'brace weight fast.csv'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "result_data = []\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(\n",
    "            base_dir, 'Datas', sub_name, condition).replace('\\\\', '/')\n",
    "        input_file_path = os.path.join(\n",
    "            input_dir, input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            third_element = int(data.iloc[1, 0])\n",
    "            last_element = int(data.iloc[-1, 0])\n",
    "            length_of_first_column = int(len(data.iloc[:, 0]))\n",
    "            result_data.append(\n",
    "                [subject, condition, third_element, last_element, length_of_first_column])\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=[\n",
    "                         'Subject', 'Condition', 'time_start', 'time_finish', 'length'])\n",
    "result_file = os.path.join(base_dir, 'result_0.xlsx')\n",
    "result_df.to_excel(result_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal_Euler.csv',\n",
    "    'walking fast_Euler.csv',\n",
    "    'weight normal_Euler.csv',\n",
    "    'weight fast_Euler.csv',\n",
    "    'brace normal_Euler.csv',\n",
    "    'brace fast_Euler.csv',\n",
    "    'brace weight normal_Euler.csv',\n",
    "    'brace weight fast_Euler.csv'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "result_data = []\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(\n",
    "            base_dir, 'Datas', sub_name, condition).replace('\\\\', '/')\n",
    "        input_file_path = os.path.join(\n",
    "            input_dir, input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            third_element = int(data.iloc[1, 0])\n",
    "            last_element = int(data.iloc[-1, 0])\n",
    "            length_of_first_column = int(len(data.iloc[:, 0]))\n",
    "            result_data.append(\n",
    "                [subject, condition, third_element, last_element, length_of_first_column])\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=[\n",
    "                         'Subject', 'Condition', 'time_start', 'time_finish', 'length'])\n",
    "result_file = os.path.join(base_dir, 'result_euler.xlsx')\n",
    "result_df.to_excel(result_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan Farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal_Joints_Kinematics.csv',\n",
    "    'walking fast_Joints_Kinematics.csv',\n",
    "    'weight normal_Joints_Kinematics.csv',\n",
    "    'weight fast_Joints_Kinematics.csv',\n",
    "    'brace normal_Joints_Kinematics.csv',\n",
    "    'brace fast_Joints_Kinematics.csv',\n",
    "    'brace weight normal_Joints_Kinematics.csv',\n",
    "    'brace weight fast_Joints_Kinematics.csv'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "result_data = []\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(\n",
    "            base_dir, 'Datas', sub_name, condition).replace('\\\\', '/')\n",
    "        input_file_path = os.path.join(\n",
    "            input_dir, input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            third_element = int(data.iloc[1, 0])\n",
    "            last_element = int(data.iloc[-1, 0])\n",
    "            length_of_first_column = int(len(data.iloc[:, 0]))\n",
    "            result_data.append(\n",
    "                [subject, condition, third_element, last_element, length_of_first_column])\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=[\n",
    "                         'Subject', 'Condition', 'time_start', 'time_finish', 'length'])\n",
    "result_file = os.path.join(base_dir, 'result_joints.xlsx')\n",
    "result_df.to_excel(result_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
