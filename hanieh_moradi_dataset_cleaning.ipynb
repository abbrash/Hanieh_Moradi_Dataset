{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw_walking normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal', \n",
    "    '2) walking fast', \n",
    "    '3) weight normal', \n",
    "    '4) weight fast',\n",
    "    '5) brace normal', \n",
    "    '6) brace fast', \n",
    "    '7) brace_weight normal', \n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'RAW_walking normal.csv', \n",
    "    'RAW_walking fast.csv', \n",
    "    'RAW_weight normal.csv', \n",
    "    'RAW_weight fast.csv',\n",
    "    'RAW_brace normal.csv', \n",
    "    'RAW_brace fast.csv', \n",
    "    'RAW_brace weight normal.csv', \n",
    "    'RAW_brace weight fast.csv'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "# Define the desired column order\n",
    "desired_column_order = [\n",
    "    'Time_LeftFoot', 'AccX_LeftFoot', 'AccY_LeftFoot', 'AccZ_LeftFoot', 'GyroX_LeftFoot', 'GyroY_LeftFoot', 'GyroZ_LeftFoot', 'MagX_LeftFoot', 'MagY_LeftFoot', 'MagZ_LeftFoot',\n",
    "    'Time_RightFoot', 'AccX_RightFoot', 'AccY_RightFoot', 'AccZ_RightFoot', 'GyroX_RightFoot', 'GyroY_RightFoot', 'GyroZ_RightFoot', 'MagX_RightFoot', 'MagY_RightFoot', 'MagZ_RightFoot',\n",
    "    'Time_LeftShank', 'AccX_LeftShank', 'AccY_LeftShank', 'AccZ_LeftShank', 'GyroX_LeftShank', 'GyroY_LeftShank', 'GyroZ_LeftShank', 'MagX_LeftShank', 'MagY_LeftShank', 'MagZ_LeftShank',\n",
    "    'Time_RightShank', 'AccX_RightShank', 'AccY_RightShank', 'AccZ_RightShank', 'GyroX_RightShank', 'GyroY_RightShank', 'GyroZ_RightShank', 'MagX_RightShank', 'MagY_RightShank', 'MagZ_RightShank',\n",
    "    'Time_LeftThigh', 'AccX_LeftThigh', 'AccY_LeftThigh', 'AccZ_LeftThigh', 'GyroX_LeftThigh', 'GyroY_LeftThigh', 'GyroZ_LeftThigh', 'MagX_LeftThigh', 'MagY_LeftThigh', 'MagZ_LeftThigh',\n",
    "    'Time_RightThigh', 'AccX_RightThigh', 'AccY_RightThigh', 'AccZ_RightThigh', 'GyroX_RightThigh', 'GyroY_RightThigh', 'GyroZ_RightThigh', 'MagX_RightThigh', 'MagY_RightThigh', 'MagZ_RightThigh',\n",
    "    'Time_LeftHumerus', 'AccX_RightHumerus', 'AccY_RightHumerus', 'AccZ_RightHumerus', 'GyroX_LeftHumerus', 'GyroY_LeftHumerus', 'GyroZ_LeftHumerus', 'MagX_LeftHumerus', 'MagY_LeftHumerus', 'MagZ_LeftHumerus',\n",
    "    'Time_RightHumerus', 'AccX_RightHumerus', 'AccY_RightHumerus', 'AccZ_RightHumerus', 'GyroX_RightHumerus', 'GyroY_RightHumerus', 'GyroZ_RightHumerus', 'MagX_RightHumerus', 'MagY_RightHumerus', 'MagZ_RightHumerus',\n",
    "    'Time_Pelvic', 'AccX_Pelvic', 'AccY_Pelvic', 'AccZ_Pelvic', 'GyroX_Pelvic', 'GyroY_Pelvic', 'GyroZ_Pelvic', 'MagX_Pelvic', 'MagY_Pelvic', 'MagZ_Pelvic',\n",
    "    'Time_Trunk', 'AccX_Trunk', 'AccY_Trunk', 'AccZ_Trunk', 'GyroX_Trunk', 'GyroY_Trunk', 'GyroZ_Trunk', 'MagX_Trunk', 'MagY_Trunk', 'MagZ_Trunk'\n",
    "]\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(base_dir, 'Data', sub_name, condition).replace('\\\\', '/')\n",
    "        output_dir = os.path.join(base_dir, 'modified data', subject, condition.replace(') ', '_'))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        input_file_path = os.path.join(input_dir, input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            first_row = pd.DataFrame(data.iloc[0]).index\n",
    "            words = list(str(word).strip() for word in first_row)\n",
    "            names = [\n",
    "                word for word in words if word and not re.search(r'\\d', word)]\n",
    "            new_col_names = []\n",
    "\n",
    "            for i in range(len(first_row)):\n",
    "                name = names[i // 10]\n",
    "                sensor = ['Time', 'AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ', 'MagX', 'MagY', 'MagZ'][i % 10]\n",
    "                new_col_names.append(f'{sensor}_{name}')\n",
    "\n",
    "            data.columns = new_col_names\n",
    "            data = data.iloc[1:]\n",
    "\n",
    "            # Sort the columns using the desired column order\n",
    "            data = data.drop_duplicates(subset=['Time_LeftFoot'], keep='first')\n",
    "            data = data.reindex(columns=desired_column_order)\n",
    "\n",
    "            output_file_name = input_file_name\n",
    "            output_file = os.path.join(output_dir, output_file_name)\n",
    "            data.to_csv(output_file, index=False)\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walking normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal.csv',\n",
    "    'walking fast.csv',\n",
    "    'weight normal.csv',\n",
    "    'weight fast.csv',\n",
    "    'brace normal.csv',\n",
    "    'brace fast.csv',\n",
    "    'brace weight normal.csv',\n",
    "    'brace weight fast.csv'\n",
    "]\n",
    "\n",
    "desired_column_order = ['Time_LeftFoot', 'Q0_LeftFoot', 'Q1_LeftFoot', 'Q2_LeftFoot', 'Q3_LeftFoot', 'Acc_X_LeftFoot', 'Acc_Y_LeftFoot', 'Acc_Z_LeftFoot', 'Acc_linX_LeftFoot', 'Acc_linY_LeftFoot', 'Acc_linZ_LeftFoot', 'Acc_GlinX_LeftFoot', 'Acc_GlinY_LeftFoot', 'Acc_GlinZ_LeftFoot',\n",
    "                        'Time_RightFoot', 'Q0_RightFoot', 'Q1_RightFoot', 'Q2_RightFoot', 'Q3_RightFoot', 'Acc_X_RightFoot', 'Acc_Y_RightFoot', 'Acc_Z_RightFoot', 'Acc_linX_RightFoot', 'Acc_linY_RightFoot', 'Acc_linZ_RightFoot', 'Acc_GlinX_RightFoot', 'Acc_GlinY_RightFoot', 'Acc_GlinZ_RightFoot',\n",
    "                        'Time_LeftShank', 'Q0_LeftShank', 'Q1_LeftShank', 'Q2_LeftShank', 'Q3_LeftShank', 'Acc_X_LeftShank', 'Acc_Y_LeftShank', 'Acc_Z_LeftShank', 'Acc_linX_LeftShank', 'Acc_linY_LeftShank', 'Acc_linZ_LeftShank', 'Acc_GlinX_LeftShank', 'Acc_GlinY_LeftShank', 'Acc_GlinZ_LeftShank',\n",
    "                        'Time_RightShank', 'Q0_RightShank', 'Q1_RightShank', 'Q2_RightShank', 'Q3_RightShank', 'Acc_X_RightShank', 'Acc_Y_RightShank', 'Acc_Z_RightShank', 'Acc_linX_RightShank', 'Acc_linY_RightShank', 'Acc_linZ_RightShank', 'Acc_GlinX_RightShank', 'Acc_GlinY_RightShank', 'Acc_GlinZ_RightShank',\n",
    "                        'Time_LeftThigh', 'Q0_LeftThigh', 'Q1_LeftThigh', 'Q2_LeftThigh', 'Q3_LeftThigh', 'Acc_X_LeftThigh', 'Acc_Y_LeftThigh', 'Acc_Z_LeftThigh', 'Acc_linX_LeftThigh', 'Acc_linY_LeftThigh', 'Acc_linZ_LeftThigh', 'Acc_GlinX_LeftThigh', 'Acc_GlinY_LeftThigh', 'Acc_GlinZ_LeftThigh',\n",
    "                        'Time_RightThigh', 'Q0_RightThigh', 'Q1_RightThigh', 'Q2_RightThigh', 'Q3_RightThigh', 'Acc_X_RightThigh', 'Acc_Y_RightThigh', 'Acc_Z_RightThigh', 'Acc_linX_RightThigh', 'Acc_linY_RightThigh', 'Acc_linZ_RightThigh', 'Acc_GlinX_RightThigh', 'Acc_GlinY_RightThigh', 'Acc_GlinZ_RightThigh',\n",
    "                        'Time_LeftHumerus', 'Q0_LeftHumerus', 'Q1_LeftHumerus', 'Q2_LeftHumerus', 'Q3_LeftHumerus', 'Acc_X_LeftHumerus', 'Acc_Y_LeftHumerus', 'Acc_Z_LeftHumerus', 'Acc_linX_LeftHumerus', 'Acc_linY_LeftHumerus', 'Acc_linZ_LeftHumerus', 'Acc_GlinX_LeftHumerus', 'Acc_GlinY_LeftHumerus', 'Acc_GlinZ_LeftHumerus',\n",
    "                        'Time_RightHumerus', 'Q0_RightHumerus', 'Q1_RightHumerus', 'Q2_RightHumerus', 'Q3_RightHumerus', 'Acc_X_RightHumerus', 'Acc_Y_RightHumerus', 'Acc_Z_RightHumerus', 'Acc_linX_RightHumerus', 'Acc_linY_RightHumerus', 'Acc_linZ_RightHumerus', 'Acc_GlinX_RightHumerus', 'Acc_GlinY_RightHumerus', 'Acc_GlinZ_RightHumerus',\n",
    "                        'Time_Pelvic', 'Q0_Pelvic', 'Q1_Pelvic', 'Q2_Pelvic', 'Q3_Pelvic', 'Acc_X_Pelvic', 'Acc_Y_Pelvic', 'Acc_Z_Pelvic', 'Acc_linX_Pelvic', 'Acc_linY_Pelvic', 'Acc_linZ_Pelvic', 'Acc_GlinX_Pelvic', 'Acc_GlinY_Pelvic', 'Acc_GlinZ_Pelvic',\n",
    "                        'Time_Trunk', 'Q0_Trunk', 'Q1_Trunk', 'Q2_Trunk', 'Q3_Trunk', 'Acc_X_Trunk', 'Acc_Y_Trunk', 'Acc_Z_Trunk', 'Acc_linX_Trunk', 'Acc_linY_Trunk', 'Acc_linZ_Trunk', 'Acc_GlinX_Trunk', 'Acc_GlinY_Trunk', 'Acc_GlinZ_Trunk'\n",
    "                        ]\n",
    "\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(\n",
    "            base_dir, 'Data', sub_name, condition).replace('\\\\', '/')\n",
    "        output_dir = os.path.join(\n",
    "            base_dir, 'modified data', subject, condition.replace(') ', '_'))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        input_file_path = os.path.join(input_dir, input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            first_row = pd.DataFrame(data.iloc[0]).index\n",
    "            words = list(str(word).strip() for word in first_row)\n",
    "            names = [\n",
    "                word for word in words if word and not re.search(r'\\d', word)]\n",
    "            new_col_names = []\n",
    "\n",
    "            for i in range(len(first_row)):\n",
    "                name = names[i // 14]\n",
    "                sensor = ['Time', 'Q0', 'Q1', 'Q2', 'Q3', 'Acc_X', 'Acc_Y', 'Acc_Z',\n",
    "                          'Acc_linX', 'Acc_linY', 'Acc_linZ', 'Acc_GlinX', 'Acc_GlinY', 'Acc_GlinZ'][i % 14]  # Get the sensor name\n",
    "                new_col_names.append(f'{sensor}_{name}')\n",
    "\n",
    "            data.columns = new_col_names\n",
    "            data = data.iloc[1:]\n",
    "\n",
    "            # Sort the columns using the desired column order\n",
    "            data = data.drop_duplicates(subset=['Time_LeftFoot'], keep='first')\n",
    "            data = data.reindex(columns=desired_column_order)\n",
    "\n",
    "            output_file_name = input_file_name\n",
    "            output_file = os.path.join(output_dir, output_file_name)\n",
    "            data.to_csv(output_file, index=False)\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walking normal Euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal_Euler.csv',\n",
    "    'walking fast_Euler.csv',\n",
    "    'weight normal_Euler.csv',\n",
    "    'weight fast_Euler.csv',\n",
    "    'brace normal_Euler.csv',\n",
    "    'brace fast_Euler.csv',\n",
    "    'brace weight normal_Euler.csv',\n",
    "    'brace weight fast_Euler.csv'\n",
    "]\n",
    "\n",
    "desired_column_order = ['Time_LeftFoot', 'Roll_LeftFoot', 'Pitch_LeftFoot', 'Yaw_LeftFoot',\n",
    "                        'Time_RightFoot', 'Roll_RightFoot', 'Pitch_RightFoot', 'Yaw_RightFoot',\n",
    "                        'Time_LeftShank', 'Roll_LeftShank', 'Pitch_LeftShank', 'Yaw_LeftShank',\n",
    "                        'Time_RightShank', 'Roll_RightShank', 'Pitch_RightShank', 'Yaw_RightShank',\n",
    "                        'Time_LeftThigh', 'Roll_LeftThigh', 'Pitch_LeftThigh', 'Yaw_LeftThigh',\n",
    "                        'Time_RightThigh', 'Roll_RightThigh', 'Pitch_RightThigh', 'Yaw_RightThigh',\n",
    "                        'Time_LeftHumerus', 'Roll_LeftHumerus', 'Pitch_LeftHumerus', 'Yaw_LeftHumerus',\n",
    "                        'Time_RightHumerus', 'Roll_RightHumerus', 'Pitch_RightHumerus', 'Yaw_RightHumerus',\n",
    "                        'Time_Pelvic', 'Roll_Pelvic', 'Pitch_Pelvic', 'Yaw_Pelvic',\n",
    "                        'Time_Trunk', 'Roll_Trunk', 'Pitch_Trunk', 'Yaw_Trunk'\n",
    "                        ]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(base_dir, 'Data', sub_name, condition).replace('\\\\', '/')\n",
    "        output_dir = os.path.join(base_dir, 'modified data', subject, condition.replace(') ', '_'))\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        input_file_path = os.path.join(input_dir, input_file_name).replace('\\\\', '/')\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            first_row = pd.DataFrame(data.iloc[0]).index\n",
    "\n",
    "            # Convert the index to a list of strings\n",
    "            words = list(str(word).strip() for word in first_row)\n",
    "\n",
    "            # Remove empty strings and strings containing numbers\n",
    "            names = [word for word in words if word and not re.search(r'\\d', word)]\n",
    "\n",
    "            # Create a list of column names\n",
    "            new_col_names = []\n",
    "            for i in range(len(first_row)):\n",
    "                name = names[i // 4]  # Get the corresponding name\n",
    "                sensor = ['Time', 'Roll', 'Pitch', 'Yaw'][i % 4]  # Get the sensor name\n",
    "                # Create the new column name\n",
    "                new_col_names.append(f'{sensor}_{name}')\n",
    "\n",
    "            # Rename the columns\n",
    "            data.columns = new_col_names\n",
    "\n",
    "            # Remove the first row\n",
    "            data = data.iloc[1:]\n",
    "            \n",
    "            # Sort the columns using the desired column order\n",
    "            data = data.drop_duplicates(subset=['Time_LeftFoot'], keep='first')\n",
    "            data = data.reindex(columns=desired_column_order)\n",
    "\n",
    "            output_file_name = input_file_name\n",
    "\n",
    "            # Construct the output file path\n",
    "            output_file = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "            # Save the updated DataFrame to the output file\n",
    "            data.to_csv(output_file, index=False)\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walking normal_Joints_Kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan Farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal', \n",
    "    '2) walking fast', \n",
    "    '3) weight normal', \n",
    "    '4) weight fast',\n",
    "    '5) brace normal', \n",
    "    '6) brace fast', \n",
    "    '7) brace_weight normal', \n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal_Joints_Kinematics.csv', \n",
    "    'walking fast_Joints_Kinematics.csv',\n",
    "    'weight normal_Joints_Kinematics.csv', \n",
    "    'weight fast_Joints_Kinematics.csv',\n",
    "    'brace normal_Joints_Kinematics.csv', \n",
    "    'brace fast_Joints_Kinematics.csv',\n",
    "    'brace weight normal_Joints_Kinematics.csv', \n",
    "    'brace weight fast_Joints_Kinematics.csv'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "# Define the desired column order\n",
    "desired_column_order = [\n",
    "    'Time_LeftAnkle', 'Abduction-Adduction_LeftAnkle', 'Internal-External Rotat_LeftAnkle', 'Flexion-Extension_LeftAnkle',\n",
    "    'Time_RightAnkle', 'Abduction-Adduction_RightAnkle', 'Internal-External Rotat_RightAnkle', 'Flexion-Extension_RightAnkle',\n",
    "    'Time_LeftKnee', 'Abduction-Adduction_LeftKnee', 'Internal-External Rotat_LeftKnee', 'Flexion-Extension_LeftKnee',\n",
    "    'Time_RightKnee', 'Abduction-Adduction_RightKnee', 'Internal-External Rotat_RightKnee', 'Flexion-Extension_RightKnee',\n",
    "    'Time_LeftHip', 'Abduction-Adduction_LeftHip', 'Internal-External Rotat_LeftHip', 'Flexion-Extension_LeftHip',\n",
    "    'Time_RightHip', 'Abduction-Adduction_RightHip', 'Internal-External Rotat_RightHip', 'Flexion-Extension_RightHip',\n",
    "    'Time_LeftShoulder', 'Abduction-Adduction_LeftShoulder', 'Internal-External Rotat_LeftShoulder', 'Flexion-Extension_LeftShoulder',\n",
    "    'Time_RightShoulder', 'Abduction-Adduction_RightShoulder', 'Internal-External Rotat_RightShoulder', 'Flexion-Extension_RightShoulder',\n",
    "    'Time_Pelvic', 'Abduction-Adduction_Pelvic', 'Internal-External Rotat_Pelvic', 'Flexion-Extension_Pelvic',\n",
    "    'Time_Trunk2Ground', 'Abduction-Adduction_Trunk2Ground', 'Internal-External Rotat_Trunk2Ground', 'Flexion-Extension_Trunk2Ground'\n",
    "]\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(\n",
    "            base_dir, 'Data', sub_name, condition).replace('\\\\', '/')\n",
    "        output_dir = os.path.join(base_dir, 'modified data', subject, condition.replace(') ', '_').replace('\\\\', '/'))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        input_file_path = os.path.join(\n",
    "            input_dir, input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            first_row = pd.DataFrame(data.iloc[0]).index\n",
    "            words = list(str(word).strip() for word in first_row)\n",
    "            names = [word for word in words if (\n",
    "                word == 'Trunk2Ground' or (word and not re.search(r'\\d', word)))]\n",
    "            new_col_names = []\n",
    "\n",
    "            for i in range(len(first_row)):\n",
    "                name = names[i // 4]\n",
    "                sensor = ['Time', 'Abduction-Adduction', 'Internal-External Rotat', 'Flexion-Extension'][i % 4]\n",
    "                new_col_names.append(f'{sensor}_{name}')\n",
    "\n",
    "            data.columns = new_col_names\n",
    "            data = data.iloc[1:]\n",
    "\n",
    "            # Sort the columns based on the desired order\n",
    "            data = data.drop_duplicates(subset=['Time_LeftAnkle'], keep='first')\n",
    "            data = data.reindex(columns=desired_column_order)\n",
    "\n",
    "            output_file_name = input_file_name\n",
    "            output_file = os.path.join(output_dir, output_file_name).replace('\\\\', '/')\n",
    "            data.to_csv(output_file, index=False)\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data - Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan Farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names_1 = [\n",
    "    'RAW_walking normal.csv',\n",
    "    'RAW_walking fast.csv',\n",
    "    'RAW_weight normal.csv',\n",
    "    'RAW_weight fast.csv',\n",
    "    'RAW_brace normal.csv',\n",
    "    'RAW_brace fast.csv',\n",
    "    'RAW_brace weight normal.csv',\n",
    "    'RAW_brace weight fast.csv'\n",
    "]\n",
    "\n",
    "\n",
    "input_file_names_2 = [\n",
    "    'walking normal.csv',\n",
    "    'walking fast.csv',\n",
    "    'weight normal.csv',\n",
    "    'weight fast.csv',\n",
    "    'brace normal.csv',\n",
    "    'brace fast.csv',\n",
    "    'brace weight normal.csv',\n",
    "    'brace weight fast.csv'\n",
    "]\n",
    "\n",
    "input_file_names_3 = [\n",
    "    'walking normal_Euler.csv',\n",
    "    'walking fast_Euler.csv',\n",
    "    'weight normal_Euler.csv',\n",
    "    'weight fast_Euler.csv',\n",
    "    'brace normal_Euler.csv',\n",
    "    'brace fast_Euler.csv',\n",
    "    'brace weight normal_Euler.csv',\n",
    "    'brace weight fast_Euler.csv'\n",
    "]\n",
    "\n",
    "input_file_names_4 = [\n",
    "    'walking normal_Joints_Kinematics.csv',\n",
    "    'walking fast_Joints_Kinematics.csv',\n",
    "    'weight normal_Joints_Kinematics.csv',\n",
    "    'weight fast_Joints_Kinematics.csv',\n",
    "    'brace normal_Joints_Kinematics.csv',\n",
    "    'brace fast_Joints_Kinematics.csv',\n",
    "    'brace weight normal_Joints_Kinematics.csv',\n",
    "    'brace weight fast_Joints_Kinematics.csv'\n",
    "]\n",
    "\n",
    "num = 7\n",
    "selected_condition = conditions[num]\n",
    "selected_file_name_1 = input_file_names_1[num]\n",
    "selected_file_name_2 = input_file_names_2[num]\n",
    "selected_file_name_3 = input_file_names_3[num]\n",
    "selected_file_name_4 = input_file_names_4[num]\n",
    "\n",
    "selected_sub = 'sub_12'\n",
    "\n",
    "base_dir = 'C:/Users/Arash/Desktop/Hanieh Moradi - Dataset/'\n",
    "\n",
    "input_dir = os.path.join(base_dir, 'modified data', selected_sub,\n",
    "                         selected_condition.replace(') ', '_').replace('\\\\', '/'))\n",
    "\n",
    "input_filename_1 = os.path.join(input_dir, selected_file_name_1).replace('\\\\', '/')\n",
    "input_filename_2 = os.path.join(input_dir, selected_file_name_2).replace('\\\\', '/')\n",
    "input_filename_3 = os.path.join(input_dir, selected_file_name_3).replace('\\\\', '/')\n",
    "input_filename_4 = os.path.join(input_dir, selected_file_name_4).replace('\\\\', '/')\n",
    "\n",
    "\n",
    "data_1 = pd.read_csv(input_filename_1)\n",
    "data_2 = pd.read_csv(input_filename_2)\n",
    "data_3 = pd.read_csv(input_filename_3)\n",
    "data_4 = pd.read_csv(input_filename_4)\n",
    "\n",
    "\n",
    "plt.plot(data_1['AccY_LeftFoot'])\n",
    "plt.show()\n",
    "plt.plot(data_1['AccY_LeftFoot'].iloc[:1000])\n",
    "plt.show()\n",
    "plt.plot(data_1['AccY_LeftFoot'].iloc[-1000:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_mod = [200:10300]\n",
    "# finish_mod =\n",
    "\n",
    "data_mod_1 = data_1.iloc[:12600]\n",
    "data_mod_1 = data_mod_1.reset_index(drop=True)\n",
    "data_mod_2 = data_2.iloc[:12600]\n",
    "data_mod_2 = data_mod_2.reset_index(drop=True)\n",
    "data_mod_3 = data_3.iloc[:12600]\n",
    "data_mod_3 = data_mod_3.reset_index(drop=True)\n",
    "data_mod_4 = data_4.iloc[:12600]\n",
    "data_mod_4 = data_mod_4.reset_index(drop=True)\n",
    "\n",
    "base_dir_mod = 'C:/Users/Arash/Desktop/Hanieh Moradi - Dataset/dataset_mod/'\n",
    "\n",
    "# Step 1: Construct the Output Directory Path\n",
    "output_dir = os.path.join(base_dir_mod, 'modified data', selected_sub, selected_condition.replace(') ', '_').replace('\\\\', '/'))\n",
    "\n",
    "# Step 2: Ensure the Directory Exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 3: Construct the Output Filename\n",
    "output_filename_1 = os.path.join(output_dir, selected_file_name_1).replace('\\\\', '/')\n",
    "output_filename_2 = os.path.join(output_dir, selected_file_name_2).replace('\\\\', '/')\n",
    "output_filename_3 = os.path.join(output_dir, selected_file_name_3).replace('\\\\', '/')\n",
    "output_filename_4 = os.path.join(output_dir, selected_file_name_4).replace('\\\\', '/')\n",
    "\n",
    "# Now, you can proceed to save your data to output_filename\n",
    "# For example, if 'data' is your DataFrame:\n",
    "data_mod_1.to_csv(output_filename_1, index=False)\n",
    "data_mod_2.to_csv(output_filename_2, index=False)\n",
    "data_mod_3.to_csv(output_filename_3, index=False)\n",
    "data_mod_4.to_csv(output_filename_4, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the elements of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'RAW_walking normal.csv',\n",
    "    'RAW_walking fast.csv',\n",
    "    'RAW_weight normal.csv',\n",
    "    'RAW_weight fast.csv',\n",
    "    'RAW_brace normal.csv',\n",
    "    'RAW_brace fast.csv',\n",
    "    'RAW_brace weight normal.csv',\n",
    "    'RAW_brace weight fast.csv'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "result_data = []\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(base_dir).replace('\\\\', '/')\n",
    "        # input_file_path = os.path.join(input_dir, input_file_name).replace('\\\\', '/')\n",
    "        input_file_path = os.path.join(input_dir, 'modified data', subject, condition.replace(\n",
    "            ') ', '_'), input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            third_element = int(data.iloc[1, 0])\n",
    "            last_element = int(data.iloc[-1, 0])\n",
    "            length_of_first_column = int(len(data.iloc[:, 0]))\n",
    "            result_data.append(\n",
    "                [subject, condition, third_element, last_element, length_of_first_column])\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=[\n",
    "                         'Subject', 'Condition', 'time_start', 'time_finish', 'length'])\n",
    "result_file = os.path.join(base_dir, 'result_raw.xlsx')\n",
    "result_df.to_excel(result_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal.csv',\n",
    "    'walking fast.csv',\n",
    "    'weight normal.csv',\n",
    "    'weight fast.csv',\n",
    "    'brace normal.csv',\n",
    "    'brace fast.csv',\n",
    "    'brace weight normal.csv',\n",
    "    'brace weight fast.csv'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "result_data = []\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(base_dir).replace('\\\\', '/')\n",
    "        # input_file_path = os.path.join(input_dir, input_file_name).replace('\\\\', '/')\n",
    "        input_file_path = os.path.join(input_dir, 'modified data', subject, condition.replace(\n",
    "            ') ', '_'), input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            third_element = int(data.iloc[1, 0])\n",
    "            last_element = int(data.iloc[-1, 0])\n",
    "            length_of_first_column = int(len(data.iloc[:, 0]))\n",
    "            result_data.append(\n",
    "                [subject, condition, third_element, last_element, length_of_first_column])\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=[\n",
    "                         'Subject', 'Condition', 'time_start', 'time_finish', 'length'])\n",
    "result_file = os.path.join(base_dir, 'result_0.xlsx')\n",
    "result_df.to_excel(result_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal_Euler.csv',\n",
    "    'walking fast_Euler.csv',\n",
    "    'weight normal_Euler.csv',\n",
    "    'weight fast_Euler.csv',\n",
    "    'brace normal_Euler.csv',\n",
    "    'brace fast_Euler.csv',\n",
    "    'brace weight normal_Euler.csv',\n",
    "    'brace weight fast_Euler.csv'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "result_data = []\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(base_dir).replace('\\\\', '/')\n",
    "        # input_file_path = os.path.join(input_dir, input_file_name).replace('\\\\', '/')\n",
    "        input_file_path = os.path.join(input_dir, 'modified data', subject, condition.replace(\n",
    "            ') ', '_'), input_file_name).replace('\\\\', '/')\n",
    "        \n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            third_element = int(data.iloc[1, 0])\n",
    "            last_element = int(data.iloc[-1, 0])\n",
    "            length_of_first_column = int(len(data.iloc[:, 0]))\n",
    "            result_data.append(\n",
    "                [subject, condition, third_element, last_element, length_of_first_column])\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=[\n",
    "                         'Subject', 'Condition', 'time_start', 'time_finish', 'length'])\n",
    "result_file = os.path.join(base_dir, 'result_euler.xlsx')\n",
    "result_df.to_excel(result_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan Farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal_Joints_Kinematics.csv',\n",
    "    'walking fast_Joints_Kinematics.csv',\n",
    "    'weight normal_Joints_Kinematics.csv',\n",
    "    'weight fast_Joints_Kinematics.csv',\n",
    "    'brace normal_Joints_Kinematics.csv',\n",
    "    'brace fast_Joints_Kinematics.csv',\n",
    "    'brace weight normal_Joints_Kinematics.csv',\n",
    "    'brace weight fast_Joints_Kinematics.csv'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "result_data = []\n",
    "\n",
    "for subject, sub_name in subjects.items():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(base_dir).replace('\\\\', '/')\n",
    "        # input_file_path = os.path.join(input_dir, input_file_name).replace('\\\\', '/')\n",
    "        input_file_path = os.path.join(input_dir, 'modified data', subject, condition.replace(\n",
    "            ') ', '_'), input_file_name).replace('\\\\', '/')\n",
    "        \n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "            third_element = int(data.iloc[1, 0])\n",
    "            last_element = int(data.iloc[-1, 0])\n",
    "            length_of_first_column = int(len(data.iloc[:, 0]))\n",
    "            result_data.append(\n",
    "                [subject, condition, third_element, last_element, length_of_first_column])\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=[\n",
    "                         'Subject', 'Condition', 'time_start', 'time_finish', 'length'])\n",
    "result_file = os.path.join(base_dir, 'result_joints.xlsx')\n",
    "result_df.to_excel(result_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the suncronization of the modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'RAW_walking normal.csv',\n",
    "    'RAW_walking fast.csv',\n",
    "    'RAW_weight normal.csv',\n",
    "    'RAW_weight fast.csv',\n",
    "    'RAW_brace normal.csv',\n",
    "    'RAW_brace fast.csv',\n",
    "    'RAW_brace weight normal.csv',\n",
    "    'RAW_brace weight fast.csv'\n",
    "]\n",
    "\n",
    "time_col = [\n",
    "    'Time_LeftFoot',\n",
    "    'Time_RightFoot',\n",
    "    'Time_LeftShank',\n",
    "    'Time_RightShank',\n",
    "    'Time_LeftThigh',\n",
    "    'Time_RightThigh',\n",
    "    'Time_LeftHumerus',\n",
    "    'Time_RightHumerus',\n",
    "    'Time_Pelvic',\n",
    "    'Time_Trunk'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "synchronization_results = []\n",
    "\n",
    "for subject in subjects.keys():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(base_dir).replace('\\\\', '/')\n",
    "        input_file_path = os.path.join(input_dir, 'modified data', subject, condition.replace(\n",
    "            ') ', '_'), input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "\n",
    "            # Get the values of the first time column as reference\n",
    "            reference_values = data[time_col[0]].values\n",
    "\n",
    "            # Check if all other time columns have identical values\n",
    "            not_synchronized_cols = []\n",
    "            for col in time_col[1:]:\n",
    "                if not data[col].values.tolist() == reference_values.tolist():\n",
    "                    not_synchronized_cols.append(col)\n",
    "\n",
    "            if not_synchronized_cols:\n",
    "                synchronization_status = \"Not Synchronized\"\n",
    "                not_synchronized_cols_str = \", \".join(not_synchronized_cols)\n",
    "            else:\n",
    "                synchronization_status = \"Synchronized\"\n",
    "                not_synchronized_cols_str = \"\"\n",
    "\n",
    "            synchronization_results.append(\n",
    "                [subjects[subject], condition, synchronization_status, not_synchronized_cols_str])\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "result_df = pd.DataFrame(synchronization_results, columns=[\n",
    "                         'Subject', 'Condition', 'Synchronization Status', 'Not Synchronized Columns'])\n",
    "result_file = os.path.join(base_dir, 'synchronization_results_raw.xlsx')\n",
    "result_df.to_excel(result_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal_Euler.csv',\n",
    "    'walking fast_Euler.csv',\n",
    "    'weight normal_Euler.csv',\n",
    "    'weight fast_Euler.csv',\n",
    "    'brace normal_Euler.csv',\n",
    "    'brace fast_Euler.csv',\n",
    "    'brace weight normal_Euler.csv',\n",
    "    'brace weight fast_Euler.csv'\n",
    "]\n",
    "\n",
    "time_col = [\n",
    "    'Time_LeftFoot',\n",
    "    'Time_RightFoot',\n",
    "    'Time_LeftShank',\n",
    "    'Time_RightShank',\n",
    "    'Time_LeftThigh',\n",
    "    'Time_RightThigh',\n",
    "    'Time_LeftHumerus',\n",
    "    'Time_RightHumerus',\n",
    "    'Time_Pelvic',\n",
    "    'Time_Trunk'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "synchronization_results = []\n",
    "\n",
    "for subject in subjects.keys():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(base_dir).replace('\\\\', '/')\n",
    "        input_file_path = os.path.join(input_dir, 'modified data', subject, condition.replace(\n",
    "            ') ', '_'), input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "\n",
    "            # Get the values of the first time column as reference\n",
    "            reference_values = data[time_col[0]].values\n",
    "\n",
    "            # Check if all other time columns have identical values\n",
    "            not_synchronized_cols = []\n",
    "            for col in time_col[1:]:\n",
    "                if not data[col].values.tolist() == reference_values.tolist():\n",
    "                    not_synchronized_cols.append(col)\n",
    "\n",
    "            if not_synchronized_cols:\n",
    "                synchronization_status = \"Not Synchronized\"\n",
    "                not_synchronized_cols_str = \", \".join(not_synchronized_cols)\n",
    "            else:\n",
    "                synchronization_status = \"Synchronized\"\n",
    "                not_synchronized_cols_str = \"\"\n",
    "\n",
    "            synchronization_results.append(\n",
    "                [subjects[subject], condition, synchronization_status, not_synchronized_cols_str])\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "result_df = pd.DataFrame(synchronization_results, columns=[\n",
    "                         'Subject', 'Condition', 'Synchronization Status', 'Not Synchronized Columns'])\n",
    "result_file = os.path.join(base_dir, 'synchronization_results_euler.xlsx')\n",
    "result_df.to_excel(result_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joints Kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal_Joints_Kinematics.csv',\n",
    "    'walking fast_Joints_Kinematics.csv',\n",
    "    'weight normal_Joints_Kinematics.csv',\n",
    "    'weight fast_Joints_Kinematics.csv',\n",
    "    'brace normal_Joints_Kinematics.csv',\n",
    "    'brace fast_Joints_Kinematics.csv',\n",
    "    'brace weight normal_Joints_Kinematics.csv',\n",
    "    'brace weight fast_Joints_Kinematics.csv'\n",
    "]\n",
    "\n",
    "time_col = [\n",
    "    'Time_LeftAnkle',\n",
    "    'Time_RightAnkle',\n",
    "    'Time_LeftKnee',\n",
    "    'Time_RightKnee',\n",
    "    'Time_LeftHip',\n",
    "    'Time_RightHip',\n",
    "    'Time_LeftShoulder',\n",
    "    'Time_RightShoulder',\n",
    "    'Time_Pelvic',\n",
    "    'Time_Trunk2Ground'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "synchronization_results = []\n",
    "\n",
    "for subject in subjects.keys():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(base_dir).replace('\\\\', '/')\n",
    "        input_file_path = os.path.join(input_dir, 'modified data', subject, condition.replace(\n",
    "            ') ', '_'), input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "\n",
    "            # Get the values of the first time column as reference\n",
    "            reference_values = data[time_col[0]].values\n",
    "\n",
    "            # Check if all other time columns have identical values\n",
    "            not_synchronized_cols = []\n",
    "            for col in time_col[1:]:\n",
    "                if not data[col].values.tolist() == reference_values.tolist():\n",
    "                    not_synchronized_cols.append(col)\n",
    "\n",
    "            if not_synchronized_cols:\n",
    "                synchronization_status = \"Not Synchronized\"\n",
    "                not_synchronized_cols_str = \", \".join(not_synchronized_cols)\n",
    "            else:\n",
    "                synchronization_status = \"Synchronized\"\n",
    "                not_synchronized_cols_str = \"\"\n",
    "\n",
    "            synchronization_results.append(\n",
    "                [subjects[subject], condition, synchronization_status, not_synchronized_cols_str])\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "result_df = pd.DataFrame(synchronization_results, columns=[\n",
    "                         'Subject', 'Condition', 'Synchronization Status', 'Not Synchronized Columns'])\n",
    "result_file = os.path.join(\n",
    "    base_dir, 'synchronization_results_joints_kinematics.xlsx')\n",
    "result_df.to_excel(result_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "subjects = {\n",
    "    'sub_01': 'Akram Shojaei',\n",
    "    'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    'sub_04': 'Farzad Moghaddam',\n",
    "    'sub_05': 'Hasti Keshavarzi',\n",
    "    'sub_06': 'Maliheh Maleki',\n",
    "    'sub_07': 'Masud Bak Khoshnevis',\n",
    "    'sub_08': 'Mobina Jamali',\n",
    "    'sub_09': 'Nadia Khalili',\n",
    "    'sub_10': 'Shahab Meghdadi',\n",
    "    'sub_11': 'Shayan farab',\n",
    "    'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    '1) walking normal',\n",
    "    '2) walking fast',\n",
    "    '3) weight normal',\n",
    "    '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    '6) brace fast',\n",
    "    '7) brace_weight normal',\n",
    "    '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    'walking normal.csv',\n",
    "    'walking fast.csv',\n",
    "    'weight normal.csv',\n",
    "    'weight fast.csv',\n",
    "    'brace normal.csv',\n",
    "    'brace fast.csv',\n",
    "    'brace weight normal.csv',\n",
    "    'brace weight fast.csv'\n",
    "]\n",
    "\n",
    "time_col = [\n",
    "    'Time_LeftFoot',\n",
    "    'Time_RightFoot',\n",
    "    'Time_LeftShank',\n",
    "    'Time_RightShank',\n",
    "    'Time_LeftThigh',\n",
    "    'Time_RightThigh',\n",
    "    'Time_LeftHumerus',\n",
    "    'Time_RightHumerus',\n",
    "    'Time_Pelvic',\n",
    "    'Time_Trunk'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "synchronization_results = []\n",
    "\n",
    "for subject in subjects.keys():\n",
    "    for condition, input_file_name in zip(conditions, input_file_names):\n",
    "        input_dir = os.path.join(base_dir).replace('\\\\', '/')\n",
    "        input_file_path = os.path.join(input_dir, 'modified data', subject, condition.replace(\n",
    "            ') ', '_'), input_file_name).replace('\\\\', '/')\n",
    "\n",
    "        if os.path.isfile(input_file_path):\n",
    "            data = pd.read_csv(input_file_path)\n",
    "\n",
    "            # Get the values of the first time column as reference\n",
    "            reference_values = data[time_col[0]].values\n",
    "\n",
    "            # Check if all other time columns have identical values\n",
    "            not_synchronized_cols = []\n",
    "            for col in time_col[1:]:\n",
    "                if not data[col].values.tolist() == reference_values.tolist():\n",
    "                    not_synchronized_cols.append(col)\n",
    "\n",
    "            if not_synchronized_cols:\n",
    "                synchronization_status = \"Not Synchronized\"\n",
    "                not_synchronized_cols_str = \", \".join(not_synchronized_cols)\n",
    "            else:\n",
    "                synchronization_status = \"Synchronized\"\n",
    "                not_synchronized_cols_str = \"\"\n",
    "\n",
    "            synchronization_results.append(\n",
    "                [subjects[subject], condition, synchronization_status, not_synchronized_cols_str])\n",
    "\n",
    "        else:\n",
    "            print(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "result_df = pd.DataFrame(synchronization_results, columns=[\n",
    "                         'Subject', 'Condition', 'Synchronization Status', 'Not Synchronized Columns'])\n",
    "result_file = os.path.join(\n",
    "    base_dir, 'synchronization_results_0.xlsx')\n",
    "result_df.to_excel(result_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulating missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "subjects = {\n",
    "    # 'sub_01': 'Akram Shojaei',\n",
    "    # 'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    # 'sub_04': 'Farzad Moghaddam',\n",
    "    # 'sub_05': 'Hasti Keshavarzi',\n",
    "    # 'sub_06': 'Maliheh Maleki',\n",
    "    # 'sub_07': 'Masud Bak Khoshnevis',\n",
    "    # 'sub_08': 'Mobina Jamali',\n",
    "    # 'sub_09': 'Nadia Khalili',\n",
    "    # 'sub_10': 'Shahab Meghdadi',\n",
    "    # 'sub_11': 'Shayan farab',\n",
    "    # 'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    # '1) walking normal',\n",
    "    # '2) walking fast',\n",
    "    # '3) weight normal',\n",
    "    # '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    # '6) brace fast',\n",
    "    # '7) brace_weight normal',\n",
    "    # '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    # 'walking normal.csv',\n",
    "    # 'walking fast.csv',\n",
    "    # 'weight normal.csv',\n",
    "    # 'weight fast.csv',\n",
    "    'brace normal.csv',\n",
    "    # 'brace fast.csv',\n",
    "    # 'brace weight normal.csv',\n",
    "    # 'brace weight fast.csv'\n",
    "]\n",
    "\n",
    "time_col = [\n",
    "    'Time_LeftFoot',\n",
    "    'Time_RightFoot',\n",
    "    'Time_LeftShank',\n",
    "    'Time_RightShank',\n",
    "    'Time_LeftThigh',\n",
    "    'Time_RightThigh',\n",
    "    'Time_LeftHumerus',\n",
    "    'Time_RightHumerus',\n",
    "    'Time_Pelvic',\n",
    "    'Time_Trunk'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "subject = 'sub_03'\n",
    "condition = '5) brace normal'\n",
    "input_file_name = 'brace normal.csv'\n",
    "\n",
    "input_dir = os.path.join(base_dir).replace('\\\\', '/')\n",
    "input_file_path = os.path.join(input_dir, 'modified data', subject, condition.replace(\n",
    "    ') ', '_'), input_file_name).replace('\\\\', '/')\n",
    "\n",
    "\n",
    "if os.path.isfile(input_file_path):\n",
    "    # Read data in chunks of 1 million rows\n",
    "    data = pd.read_csv(input_file_path, chunksize=1000)\n",
    "\n",
    "    updated_data = []  # Initialize an empty list to store updated chunks\n",
    "    not_synchronized_cols = []  # Initialize a list to store not synchronized columns\n",
    "\n",
    "    for chunk in data:\n",
    "        # Get the values of the first time column as reference\n",
    "        reference_values = chunk[time_col[0]].values\n",
    "        reference_start_time = reference_values[0]\n",
    "        reference_end_time = reference_values[-1]\n",
    "\n",
    "        # Check if all other time columns have the same start and end times as the reference\n",
    "        for col in time_col[1:]:\n",
    "            col_values = chunk[col].values\n",
    "            col_start_time = col_values[0]\n",
    "            col_end_time = col_values[-1]\n",
    "\n",
    "            if col_start_time != reference_start_time or col_end_time != reference_end_time:\n",
    "                not_synchronized_cols.append(col)\n",
    "\n",
    "        # If there are not synchronized columns, update the data\n",
    "        if not_synchronized_cols:\n",
    "            # Create a list of expected time values based on the sampling rate\n",
    "            expected_times = np.arange(\n",
    "                reference_start_time, reference_end_time + 0.006, 0.006)\n",
    "\n",
    "            # Check if all time columns have the expected time values\n",
    "            for col in not_synchronized_cols:\n",
    "                # Create a boolean mask for missing time values\n",
    "                mask = ~np.isin(expected_times, chunk[col].values)\n",
    "\n",
    "                # Insert rows with missing time values and fill with NaN\n",
    "                missing_times = expected_times[mask]\n",
    "                missing_rows = pd.DataFrame(\n",
    "                    {col: missing_times}, index=missing_times)\n",
    "                missing_rows = missing_rows.reindex(\n",
    "                    expected_times, fill_value=np.nan)\n",
    "                chunk = pd.concat([chunk, missing_rows],\n",
    "                                  axis=0, ignore_index=True)\n",
    "                chunk = chunk.sort_values(by=col).reset_index(drop=True)\n",
    "\n",
    "        updated_data.append(chunk)\n",
    "        not_synchronized_cols = []  # Reset the list for the next chunk\n",
    "\n",
    "    # Concatenate the updated chunks\n",
    "    updated_data = pd.concat(updated_data, ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    output_file_path = os.path.join(input_dir, 'modified data', subject, condition.replace(\n",
    "        ') ', '_'), f\"{input_file_name.split('.')[0]}_updated.csv\")\n",
    "    updated_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "else:\n",
    "    print(f\"Input file not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "subjects = {\n",
    "    # 'sub_01': 'Akram Shojaei',\n",
    "    # 'sub_02': 'Ali Aghapour',\n",
    "    'sub_03': 'Behnaz Behara',\n",
    "    # 'sub_04': 'Farzad Moghaddam',\n",
    "    # 'sub_05': 'Hasti Keshavarzi',\n",
    "    # 'sub_06': 'Maliheh Maleki',\n",
    "    # 'sub_07': 'Masud Bak Khoshnevis',\n",
    "    # 'sub_08': 'Mobina Jamali',\n",
    "    # 'sub_09': 'Nadia Khalili',\n",
    "    # 'sub_10': 'Shahab Meghdadi',\n",
    "    # 'sub_11': 'Shayan farab',\n",
    "    # 'sub_12': 'Soroush Rezaei'\n",
    "}\n",
    "\n",
    "conditions = [\n",
    "    # '1) walking normal',\n",
    "    # '2) walking fast',\n",
    "    # '3) weight normal',\n",
    "    # '4) weight fast',\n",
    "    '5) brace normal',\n",
    "    # '6) brace fast',\n",
    "    # '7) brace_weight normal',\n",
    "    # '8) brace_weight fast'\n",
    "]\n",
    "\n",
    "input_file_names = [\n",
    "    # 'walking normal.csv',\n",
    "    # 'walking fast.csv',\n",
    "    # 'weight normal.csv',\n",
    "    # 'weight fast.csv',\n",
    "    'brace normal.csv',\n",
    "    # 'brace fast.csv',\n",
    "    # 'brace weight normal.csv',\n",
    "    # 'brace weight fast.csv'\n",
    "]\n",
    "\n",
    "time_col = [\n",
    "    'Time_LeftFoot',\n",
    "    'Time_RightFoot',\n",
    "    'Time_LeftShank',\n",
    "    'Time_RightShank',\n",
    "    'Time_LeftThigh',\n",
    "    'Time_RightThigh',\n",
    "    'Time_LeftHumerus',\n",
    "    'Time_RightHumerus',\n",
    "    'Time_Pelvic',\n",
    "    'Time_Trunk'\n",
    "]\n",
    "\n",
    "base_dir = 'D:/Hanieh Moradi - Dataset/'\n",
    "\n",
    "subject = 'sub_03'\n",
    "condition = '5) brace normal'\n",
    "input_file_name = 'brace normal.csv'\n",
    "\n",
    "input_dir = os.path.join(base_dir).replace('\\\\', '/')\n",
    "input_file_path = os.path.join(input_dir, 'modified data', subject, condition.replace(\n",
    "    ') ', '_'), input_file_name).replace('\\\\', '/')\n",
    "\n",
    "if os.path.isfile(input_file_path):\n",
    "    # Read the entire dataset at once\n",
    "    data = pd.read_csv(input_file_path)\n",
    "\n",
    "    updated_data = data.copy()  # Start with a copy of the original data\n",
    "    not_synchronized_cols = []\n",
    "\n",
    "    # Reference time column\n",
    "    reference_col = time_col[0]  # Using Time_LeftFoot as reference\n",
    "\n",
    "    # Check for synchronization across all time columns\n",
    "    for col in time_col[1:]:\n",
    "        if col!= reference_col:\n",
    "            not_synchronized_cols.append(col)\n",
    "\n",
    "    # If there are not synchronized columns, update the data\n",
    "    if not_synchronized_cols:\n",
    "        # Create a list of expected time values based on the sampling rate\n",
    "        expected_times = np.arange(data[reference_col].min(), data[reference_col].max() + 0.006, 0.006)\n",
    "\n",
    "        # Update each not synchronized column\n",
    "        for col in not_synchronized_cols:\n",
    "            # Find missing time values\n",
    "            missing_times = [t for t in expected_times if t not in data[col].values]\n",
    "\n",
    "            # Insert rows with missing time values and fill with NaN\n",
    "            if missing_times:\n",
    "                missing_rows = pd.DataFrame({col: missing_times})\n",
    "                updated_data = pd.concat([updated_data, missing_rows], ignore_index=True)\n",
    "                updated_data = updated_data.sort_values(by=col).reset_index(drop=True)\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    output_file_path = os.path.join(input_dir, 'modified data', subject, condition.replace(\n",
    "        ') ', '_'), f\"{input_file_name.split('.')[0]}_updated.csv\")\n",
    "    updated_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "else:\n",
    "    print(f\"Input file not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test my new approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Creating a new dataset with gaps in the time series\n",
    "time_mod1 = np.array(range(0, 10)).astype(int)\n",
    "time_mod2 = np.array([5, 6, 7, 8, 9, 10, 10, 10, 13, 15]).astype(int)\n",
    "time_mod3 = np.array(range(0, 10)).astype(int)\n",
    "\n",
    "acc_x_mod1 = random.random() * np.array(range(0, 10))\n",
    "acc_y_mod1 = random.random() * np.array(range(0, 10))\n",
    "acc_z_mod1 = random.random() * np.array(range(0, 10))\n",
    "\n",
    "acc_x_mod2 = random.random() * np.array(range(0, 10))\n",
    "acc_y_mod2 = random.random() * np.array(range(0, 10))\n",
    "acc_z_mod2 = random.random() * np.array(range(0, 10))\n",
    "\n",
    "acc_x_mod3 = random.random() * np.array(range(0, 10))\n",
    "acc_y_mod3 = random.random() * np.array(range(0, 10))\n",
    "acc_z_mod3 = random.random() * np.array(range(0, 10))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'time_mod1': time_mod1, 'acc_x_mod1': acc_x_mod1, 'acc_y_mod1': acc_y_mod1, 'acc_z_mod1': acc_z_mod1,\n",
    "    'time_mod2': time_mod2, 'acc_x_mod2': acc_x_mod2, 'acc_y_mod2': acc_y_mod2, 'acc_z_mod2': acc_z_mod2,\n",
    "    'time_mod3': time_mod3, 'acc_x_mod3': acc_x_mod3, 'acc_y_mod3': acc_y_mod3, 'acc_z_mod3': acc_z_mod3\n",
    "})\n",
    "\n",
    "# Function to remove duplicates in time columns\n",
    "\n",
    "\n",
    "def remove_duplicates(df, time_col_prefix='time_'):\n",
    "    df_cleaned = df.copy()\n",
    "    for col in df.columns:\n",
    "        if col.startswith(time_col_prefix):\n",
    "            module_prefix = col.split(time_col_prefix)[1]\n",
    "            unique_time = df[col].drop_duplicates(keep='first')\n",
    "            df_cleaned = df_cleaned[df_cleaned[col].isin(\n",
    "                unique_time)].reset_index(drop=True)\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "# Remove duplicates for time columns\n",
    "df_cleaned = remove_duplicates(df)\n",
    "\n",
    "# Get all time columns\n",
    "time_columns = [col for col in df_cleaned.columns if col.startswith('time_')]\n",
    "\n",
    "# Get the overall min and max time values across all time columns\n",
    "min_time = min(df_cleaned[time_columns].apply(\n",
    "    lambda x: x.dropna().min()).values)\n",
    "max_time = max(df_cleaned[time_columns].apply(\n",
    "    lambda x: x.dropna().max()).values)\n",
    "\n",
    "# Create a common time range\n",
    "common_time = pd.Series(np.arange(min_time, max_time + 1))\n",
    "\n",
    "# Initialize a dictionary to store the synchronized data\n",
    "synchronized_data = {'time_common': common_time}\n",
    "\n",
    "# Function to interpolate the data\n",
    "\n",
    "\n",
    "def interpolate_column(time_series, data_series, common_time):\n",
    "    data_interp = np.full_like(common_time, np.nan, dtype=np.double)\n",
    "    mask = np.isin(common_time, time_series)\n",
    "    data_interp[mask] = np.interp(common_time[mask], time_series, data_series)\n",
    "    return data_interp\n",
    "\n",
    "\n",
    "# Interpolate each module's data\n",
    "for col in df_cleaned.columns:\n",
    "    if col.startswith('time_'):\n",
    "        module_prefix = col.split('time_')[1]\n",
    "        time_series = df_cleaned[col].dropna().values\n",
    "        acc_x_series = df_cleaned[f'acc_x_{module_prefix}'].dropna().values\n",
    "        acc_y_series = df_cleaned[f'acc_y_{module_prefix}'].dropna().values\n",
    "        acc_z_series = df_cleaned[f'acc_z_{module_prefix}'].dropna().values\n",
    "\n",
    "        synchronized_data[f'time_{module_prefix}'] = common_time\n",
    "        synchronized_data[f'acc_x_{module_prefix}'] = interpolate_column(\n",
    "            time_series, acc_x_series, common_time)\n",
    "        synchronized_data[f'acc_y_{module_prefix}'] = interpolate_column(\n",
    "            time_series, acc_y_series, common_time)\n",
    "        synchronized_data[f'acc_z_{module_prefix}'] = interpolate_column(\n",
    "            time_series, acc_z_series, common_time)\n",
    "\n",
    "# Convert the synchronized data to a DataFrame\n",
    "synchronized_df = pd.DataFrame(synchronized_data)\n",
    "\n",
    "# Display the synchronized DataFrame\n",
    "print(synchronized_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
